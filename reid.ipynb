{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "from utils.utils import get_yolo_boxes, preprocess_input, decode_netout, correct_yolo_boxes, do_nms\n",
    "from utils.bbox import draw_boxes\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Dropout, Flatten, Dense,Input\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import RandomNormal\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    def __init__(self, feature_vector, class_id, image_data=None):\n",
    "        self.feature_vector = feature_vector\n",
    "        self.class_id = class_id\n",
    "        self.image_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_learning_phase(0)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "anchors = [10,37, 17,71, 28,104, 28,50, 42,79, 45,148, 70,92, 77,181, 193,310]\n",
    "labels = ['person']\n",
    "net_size = 608\n",
    "obj_thresh, nms_thresh = 0.9, 0.4\n",
    "snapshot_name = 'snapshots/current_person/yolo3_model.h5'\n",
    "every_nth = 5\n",
    "\n",
    "if not os.path.exists(snapshot_name):\n",
    "    raise FileNotFoundError(snapshot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:270: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from yolo import RegressionLayer\n",
    "\n",
    "infer_model = load_model(snapshot_name, custom_objects={'RegressionLayer': RegressionLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys not found in destination state_dict: \n",
      "\t state_dicts\n",
      "\t scores\n",
      "\t ep\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from resnet import resnet50\n",
    "\n",
    "def load_state_dict(model, src_state_dict):\n",
    "    from torch.nn import Parameter\n",
    "\n",
    "    dest_state_dict = model.state_dict()\n",
    "    for name, param in src_state_dict.items():\n",
    "        if name not in dest_state_dict:\n",
    "            continue\n",
    "        if isinstance(param, Parameter):\n",
    "            param = param.data\n",
    "        try:\n",
    "            dest_state_dict[name].copy_(param)\n",
    "        except:\n",
    "            print(\"Warning: Error occurs when copying '{}': {}\".format(name, str(msg)))\n",
    "\n",
    "    src_missing = set(dest_state_dict.keys()) - set(src_state_dict.keys())\n",
    "    if len(src_missing) > 0:\n",
    "        # print(\"Keys not found in source state_dict: \")\n",
    "        pass\n",
    "    for n in src_missing:\n",
    "        # print('\\t', n)\n",
    "        pass\n",
    "\n",
    "    dest_missing = set(src_state_dict.keys()) - set(dest_state_dict.keys())\n",
    "    if len(dest_missing) > 0:\n",
    "        print(\"Keys not found in destination state_dict: \")\n",
    "        for n in dest_missing:\n",
    "            print('\\t', n)\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "\n",
    "class FeatureModel(nn.Module):\n",
    "    def __init__(self, local_conv_out_channels=128):\n",
    "        super(FeatureModel, self).__init__()\n",
    "        self.base = resnet50(pretrained=True)\n",
    "        planes = 2048\n",
    "        self.local_conv = nn.Conv2d(planes, local_conv_out_channels, 1)\n",
    "        self.local_bn = nn.BatchNorm2d(local_conv_out_channels)\n",
    "        self.local_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape [N, C, H, W]\n",
    "        feat = self.base(x)\n",
    "        global_feat = F.avg_pool2d(feat, feat.size()[2:])\n",
    "        # shape [N, C]\n",
    "        global_feat = global_feat.view(global_feat.size(0), -1)\n",
    "        # shape [N, C, H, 1]\n",
    "        local_feat = torch.mean(feat, -1, keepdim=True)\n",
    "        local_feat = self.local_relu(self.local_bn(self.local_conv(local_feat)))\n",
    "        # shape [N, H, c]\n",
    "        local_feat = local_feat.squeeze(-1).permute(0, 2, 1)\n",
    "\n",
    "        return global_feat, local_feat\n",
    "    \n",
    "    \n",
    "def get_feature(model, images):\n",
    "    ims = torch.autograd.Variable(torch.from_numpy(images).float())\n",
    "    global_feat, local_feat = model(ims)[:2]\n",
    "    return global_feat.detach().numpy()\n",
    "\n",
    "\n",
    "feature_model = FeatureModel()\n",
    "checkpoint = '/home/svakhreev/PycharmProjects/AlignedReID-Re-Production-Pytorch/snapshots/ckpt.pth'\n",
    "sd = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "load_state_dict(feature_model, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_reader = cv2.VideoCapture('/mnt/nfs/Videos/peoples_videos/outdoor/bright_by_moscow_time/primorskiy_bulvar.ts.mp4')\n",
    "\n",
    "all_features = []\n",
    "obj_thresh, nms_thresh = 0.9, 0.4\n",
    "\n",
    "def cut_boxes(image, boxes, obj_thresh):\n",
    "    boxes_imgs = []\n",
    "    for box in boxes:\n",
    "        boxes_imgs.append(image[box.ymin:box.ymax, box.xmin:box.xmax, :])\n",
    "    return boxes_imgs\n",
    "\n",
    "frames_processed = 0\n",
    "cv2.namedWindow('image', cv2.WINDOW_KEEPRATIO)\n",
    "while True:\n",
    "    read, image = video_reader.read()\n",
    "    if not read:\n",
    "        break\n",
    "\n",
    "    frames_processed += 1\n",
    "\n",
    "    if frames_processed % every_nth != 0:\n",
    "        continue\n",
    "\n",
    "    boxes = get_yolo_boxes(infer_model, image, net_size, net_size, obj_thresh, nms_thresh)\n",
    "    cv2.imshow('image', draw_boxes(image, boxes, labels, obj_thresh))\n",
    "    key = cv2.waitKeyEx(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "    \n",
    "    boxes = [box for box in boxes if box.classes[0] > obj_thresh]\n",
    "    boxes = [box for box in boxes if box.xmin > 0 and box.ymin > 0]\n",
    "    cutted_boxes = cut_boxes(image, boxes, obj_thresh)\n",
    "    data = np.zeros((len(cutted_boxes), 3, 256, 128), dtype=np.float32)\n",
    "    for i, box in enumerate(cutted_boxes):\n",
    "        box = cv2.cvtColor(box, cv2.COLOR_BGR2RGB)\n",
    "        box = cv2.resize(box, (128, 256))\n",
    "        box = box / 255.\n",
    "        data[i] = box.transpose(2, 0, 1)\n",
    "    for i in range((len(data) // 8) + 1):\n",
    "        if len(data[i*8:i*8+8]) == 0:\n",
    "            continue\n",
    "        features = get_feature(feature_model, data[i*8:i*8+8])\n",
    "        for j, f in enumerate(features):\n",
    "            all_features.append(Feature(f, 0, image_data=cutted_boxes[i * 8 + j]))\n",
    "cv2.destroyAllWindows()\n",
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(array1, array2, type='euclidean'):\n",
    "    assert type in ['cosine', 'euclidean']\n",
    "    if type == 'cosine':\n",
    "        array1 = normalize(array1, axis=1)\n",
    "        array2 = normalize(array2, axis=1)\n",
    "        dist = np.matmul(array1, array2.T)\n",
    "        return dist\n",
    "    else:\n",
    "        # shape [m1, 1]\n",
    "        square1 = np.sum(np.square(array1), axis=1)[..., np.newaxis]\n",
    "        # shape [1, m2]\n",
    "        square2 = np.sum(np.square(array2), axis=1)[np.newaxis, ...]\n",
    "        squared_dist = - 2 * np.matmul(array1, array2.T) + square1 + square2\n",
    "        squared_dist[squared_dist < 0] = 0\n",
    "        dist = np.sqrt(squared_dist)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def flat( *nums ):\n",
    "    return tuple( int(round(n)) for n in nums )\n",
    "\n",
    "class Size(object):\n",
    "    def __init__(self, pair):\n",
    "        self.width = float(pair[0])\n",
    "        self.height = float(pair[1])\n",
    "\n",
    "    @property\n",
    "    def aspect_ratio(self):\n",
    "        return self.width / self.height\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return flat(self.width, self.height)\n",
    "\n",
    "def cropped_thumbnail(img, size):\n",
    "    original = Size(img.size)\n",
    "    target = Size(size)\n",
    "\n",
    "    if target.aspect_ratio > original.aspect_ratio:\n",
    "        scale_factor = target.width / original.width\n",
    "        crop_size = Size( (original.width, target.height / scale_factor) )\n",
    "        top_cut_line = (original.height - crop_size.height) / 2\n",
    "        img = img.crop( flat(0, top_cut_line, crop_size.width, top_cut_line + crop_size.height) )\n",
    "    elif target.aspect_ratio < original.aspect_ratio:\n",
    "        scale_factor = target.height / original.height\n",
    "        crop_size = Size( (target.width/scale_factor, original.height) )\n",
    "        side_cut_line = (original.width - crop_size.width) / 2\n",
    "        img = img.crop( flat(side_cut_line, 0,  side_cut_line + crop_size.width, crop_size.height) )\n",
    "        \n",
    "    return img.resize(target.size, Image.ANTIALIAS)\n",
    "\n",
    "\n",
    "def get_master_size_and_grid(image_size, images_count):\n",
    "    w, h = image_size\n",
    "    for i in range(100):\n",
    "        if (i * i) > images_count:\n",
    "            return (w * i, h * i), i\n",
    "\n",
    "        \n",
    "def create_sprite(path, images, size=(150, 150)):\n",
    "    images = [Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 'RGB') for image in images]\n",
    "    images = [cropped_thumbnail(image, size) for image in images]\n",
    "\n",
    "    master_size, grid = get_master_size_and_grid(size, len(images))\n",
    "\n",
    "    master = Image.new(\n",
    "        mode='RGBA',\n",
    "        size=(master_size[0], master_size[1]),\n",
    "        color=(0, 0, 0, 0))\n",
    "\n",
    "    image_index = 0\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            if image_index >= len(images):\n",
    "                current_image = Image.new('RGBA', size, color=(0, 0, 0, 0))\n",
    "            else:\n",
    "                current_image = images[image_index] \n",
    "            master.paste(current_image, (j * size[1], i * size[0]))\n",
    "            image_index += 1\n",
    "        \n",
    "    master.save(os.path.join(path, 'sprite.png'))\n",
    "    return os.path.join(path, 'sprite.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/model.ckpt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import shutil\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "log_dir = './logs/'\n",
    "test_images_dir = os.path.join(log_dir, 'test_images')\n",
    "\n",
    "session = K.get_session()\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "if not os.path.exists(test_images_dir):\n",
    "    os.mkdir(test_images_dir)\n",
    "else:\n",
    "    shutil.rmtree(log_dir)\n",
    "    os.mkdir(log_dir)\n",
    "    os.mkdir(test_images_dir)\n",
    "\n",
    "embedding_var = tf.Variable(np.array([f.feature_vector for f in all_features]),\n",
    "                            name='images_embedding', dtype=tf.float32)\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "sprite_element_size = (50, 50)\n",
    "create_sprite(test_images_dir, [f.image_data for f in all_features], size=sprite_element_size)\n",
    "embedding.sprite.image_path = '/home/svakhreev/PycharmProjects/keras-yolo3/logs/test_images/sprite.png'\n",
    "embedding.sprite.single_image_dim.extend(sprite_element_size)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(log_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(session, os.path.join(log_dir, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
